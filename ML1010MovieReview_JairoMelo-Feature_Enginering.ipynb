{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Movie Reviews\n",
    "This project will analyze whether the reviews of a movie is positive and negative based using IMDB reviews.  The goal is to create a model that will analyze a text and will classify it a negative or positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary dependencies and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import numpy as np\n",
    "#import tensorflow as tf\n",
    "\n",
    "#Using Matplot to draw the Wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_selection import SelectKBest\n",
    "#from sklearn.feature_selection import f_classif\n",
    "\n",
    "#from tensorflow.python.keras import models\n",
    "#from tensorflow.python.keras.layers import Dense\n",
    "#from tensorflow.python.keras.layers import Dropout\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "There are 5 files:\n",
    "1. Train Texts\n",
    "2. Train Labels\n",
    "3. Test Texts\n",
    "4. Test Labels\n",
    "5. Dev/Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_path = '/Users/jairomelo/Desktop/ML/YORK/ML1010/FinalProject/txtImdb'\n",
    "\n",
    "# Load the dataset\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for dset in ['train', 'test']:\n",
    "    for cat in ['pos', 'neg']:\n",
    "        dset_path = os.path.join(imdb_path, dset, cat)\n",
    "        for fname in sorted(os.listdir(dset_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(dset_path, fname)) as f:\n",
    "                    if dset == 'train': train_texts.append(f.read())\n",
    "                    else: test_texts.append(f.read())\n",
    "                label = 0 if cat == 'neg' else 1\n",
    "                if dset == 'train': train_labels.append(label)\n",
    "                else: test_labels.append(label)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Development\n",
    "\n",
    "dev_imdb_path = '/Users/jairomelo/Desktop/ML/YORK/ML1010/FinalProject/txtImdb/train'\n",
    "\n",
    "dev_texts = []\n",
    "for fname in sorted(os.listdir(dev_imdb_path)):\n",
    "    if fname.endswith('.txt'):\n",
    "        with open(os.path.join(dev_imdb_path, fname)) as f:\n",
    "            dev_texts.append(f.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_new = {\n",
    "    'text': train_texts,\n",
    "    'label': train_labels\n",
    "}\n",
    "\n",
    "\n",
    "data_train = pd.DataFrame(dict_new)\n",
    "\n",
    "\n",
    "dict_new = {\n",
    "    'text': test_texts,\n",
    "    'label': test_labels\n",
    "}\n",
    "\n",
    "data_test = pd.DataFrame(dict_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove URL's from train and test\n",
    "data_train['clean_text'] = data_train['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "data_test['clean_text'] = data_test['text'].apply(lambda x: re.sub(r'http\\S+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation marks\n",
    "punctuation = '!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
    "data_train['clean_text'] = data_train['clean_text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\n",
    "data_test['clean_text'] = data_test['clean_text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\n",
    "\n",
    "# convert text to lowercase\n",
    "data_train['clean_text'] = data_train['clean_text'].str.lower()\n",
    "data_test['clean_text'] = data_test['clean_text'].str.lower()\n",
    "\n",
    "# remove numbers\n",
    "data_train['clean_text'] = data_train['clean_text'].str.replace(\"[0-9]\", \" \")\n",
    "data_test['clean_text'] = data_test['clean_text'].str.replace(\"[0-9]\", \" \")\n",
    "\n",
    "# remove whitespaces\n",
    "data_train['clean_text'] = data_train['clean_text'].apply(lambda x:' '.join(x.split()))\n",
    "data_test['clean_text'] = data_test['clean_text'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data_train['len'] = data_train['text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data_train['punct%'] = data_train['text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "data_test['len'] = data_test['text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data_test['punct%'] = data_test['text'].apply(lambda x: count_punct(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "data_train['clean_text'] = data_train['clean_text'].apply(lambda x: tokenize(x.lower()))\n",
    "data_test['clean_text'] = data_test['clean_text'].apply(lambda x: tokenize(x.lower()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data_train['clean_text'] = data_train['clean_text'].apply(lambda x: remove_stopwords(x))\n",
    "data_test['clean_text'] = data_test['clean_text'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>len</th>\n",
       "      <th>punct%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19933</th>\n",
       "      <td>I have been a huge Lynn Peterson fan ever sinc...</td>\n",
       "      <td>0</td>\n",
       "      <td>[huge, lynn, peterson, fan, ever, since, break...</td>\n",
       "      <td>660</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>Being the prototype of the classical Errol Fly...</td>\n",
       "      <td>1</td>\n",
       "      <td>[prototype, classical, errol, flynn, adventure...</td>\n",
       "      <td>277</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10777</th>\n",
       "      <td>Was'nt really bad for Raw's first PPV of 006. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[nt, really, bad, raw, first, ppv, ending, rea...</td>\n",
       "      <td>2735</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22588</th>\n",
       "      <td>So far Nightmares and Dreamscapes has been err...</td>\n",
       "      <td>0</td>\n",
       "      <td>[far, nightmares, dreamscapes, erratic, disapp...</td>\n",
       "      <td>1125</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5962</th>\n",
       "      <td>Most people, when they think of expressionist ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[people, think, expressionist, cinema, look, b...</td>\n",
       "      <td>2134</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8494</th>\n",
       "      <td>You think you've had it tough? You should chec...</td>\n",
       "      <td>1</td>\n",
       "      <td>[think, tough, check, film, carl, brashear, ep...</td>\n",
       "      <td>516</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>Franco proves, once again, that he is the prin...</td>\n",
       "      <td>1</td>\n",
       "      <td>[franco, proves, prince, surreal, erotic, cine...</td>\n",
       "      <td>670</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9266</th>\n",
       "      <td>A linear travel within a non-linear structure....</td>\n",
       "      <td>1</td>\n",
       "      <td>[linear, travel, within, nonlinear, structure,...</td>\n",
       "      <td>2874</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24144</th>\n",
       "      <td>i should qualify that title, now that i think ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[qualify, title, think, checkout, entirely, wo...</td>\n",
       "      <td>1309</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21777</th>\n",
       "      <td>This movie never made it to theaters in our ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>[movie, never, made, theaters, area, became, a...</td>\n",
       "      <td>620</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "19933  I have been a huge Lynn Peterson fan ever sinc...      0   \n",
       "1853   Being the prototype of the classical Errol Fly...      1   \n",
       "10777  Was'nt really bad for Raw's first PPV of 006. ...      1   \n",
       "22588  So far Nightmares and Dreamscapes has been err...      0   \n",
       "5962   Most people, when they think of expressionist ...      1   \n",
       "8494   You think you've had it tough? You should chec...      1   \n",
       "4369   Franco proves, once again, that he is the prin...      1   \n",
       "9266   A linear travel within a non-linear structure....      1   \n",
       "24144  i should qualify that title, now that i think ...      0   \n",
       "21777  This movie never made it to theaters in our ar...      0   \n",
       "\n",
       "                                              clean_text   len  punct%  \n",
       "19933  [huge, lynn, peterson, fan, ever, since, break...   660    13.3  \n",
       "1853   [prototype, classical, errol, flynn, adventure...   277     5.1  \n",
       "10777  [nt, really, bad, raw, first, ppv, ending, rea...  2735     5.5  \n",
       "22588  [far, nightmares, dreamscapes, erratic, disapp...  1125     4.2  \n",
       "5962   [people, think, expressionist, cinema, look, b...  2134     5.2  \n",
       "8494   [think, tough, check, film, carl, brashear, ep...   516     4.3  \n",
       "4369   [franco, proves, prince, surreal, erotic, cine...   670     2.5  \n",
       "9266   [linear, travel, within, nonlinear, structure,...  2874     7.5  \n",
       "24144  [qualify, title, think, checkout, entirely, wo...  1309     3.7  \n",
       "21777  [movie, never, made, theaters, area, became, a...   620     6.8  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to np.array\n",
    "train = np.array(data_train)\n",
    "test = np.array(data_test)\n",
    "dev = np.array(dev_texts)\n",
    "\n",
    "import pandas as pd \n",
    "pd.DataFrame(train).to_csv(\"train.txt\")\n",
    "pd.DataFrame(test).to_csv(\"test.txt\")\n",
    "# Converting to np.array\n",
    "dev = np.array(dev_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "2019-04-17 14:35:05,315 Reading data from /Users/jairomelo/Desktop/ML/YORK/ML1010/FinalProject\n",
      "2019-04-17 14:35:05,316 Train: /Users/jairomelo/Desktop/ML/YORK/ML1010/FinalProject/train.txt\n",
      "2019-04-17 14:35:05,316 Dev: /Users/jairomelo/Desktop/ML/YORK/ML1010/FinalProject/dev.txt\n",
      "2019-04-17 14:35:05,317 Test: /Users/jairomelo/Desktop/ML/YORK/ML1010/FinalProject/test.txt\n"
     ]
    }
   ],
   "source": [
    "#Loading the clean data and corpus\n",
    "\n",
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'clean_text', 1: 'label', 2: 'len', 3: 'punct'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '/Users/jairomelo/Desktop/ML/YORK/ML1010/FinalProject'\n",
    "# retrieve corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns,\n",
    "                                                              train_file='train.txt',\n",
    "                                                              test_file='test.txt',\n",
    "                                                              dev_file='dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings\n",
    "from typing import List\n",
    "\n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    WordEmbeddings('glove')\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-17 14:35:51,980 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:35:51,981 Evaluation method: MICRO_F1_SCORE\n",
      "2019-04-17 14:35:51,983 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:39:16,312 epoch 1 - iter 0/1 - loss 16444.18750000\n",
      "2019-04-17 14:39:16,331 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:39:16,332 EPOCH 1 done: loss 16444.1875 - lr 0.1000 - bad epochs 0\n",
      "2019-04-17 14:39:26,277 DEV  : loss 4853.42578125 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 14:39:35,853 TEST : loss 4942.63085938 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 14:39:40,303 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:43:04,148 epoch 2 - iter 0/1 - loss 4786.60937500\n",
      "2019-04-17 14:43:04,165 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:43:04,165 EPOCH 2 done: loss 4786.6094 - lr 0.1000 - bad epochs 0\n",
      "2019-04-17 14:43:13,572 DEV  : loss 1072.27343750 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 14:43:22,841 TEST : loss 1077.41796875 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 14:43:27,131 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:47:00,520 epoch 3 - iter 0/1 - loss 1101.98046875\n",
      "2019-04-17 14:47:00,540 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:47:00,541 EPOCH 3 done: loss 1101.9805 - lr 0.1000 - bad epochs 0\n",
      "2019-04-17 14:47:10,768 DEV  : loss 259.58593750 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 14:47:20,434 TEST : loss 239.86718750 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 14:47:24,941 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:51:01,463 epoch 4 - iter 0/1 - loss 227.14062500\n",
      "2019-04-17 14:51:01,483 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:51:01,484 EPOCH 4 done: loss 227.1406 - lr 0.1000 - bad epochs 0\n",
      "2019-04-17 14:51:11,715 DEV  : loss 68.94531250 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 14:51:21,863 TEST : loss 41.49218750 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 14:51:26,280 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:54:54,445 epoch 5 - iter 0/1 - loss 64.36718750\n",
      "2019-04-17 14:54:54,461 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:54:54,462 EPOCH 5 done: loss 64.3672 - lr 0.1000 - bad epochs 0\n",
      "2019-04-17 14:55:04,556 DEV  : loss 3.32031250 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 14:55:14,834 TEST : loss -12.82812500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 14:55:19,774 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:58:15,815 epoch 6 - iter 0/1 - loss -4.28125000\n",
      "2019-04-17 14:58:15,830 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 14:58:15,831 EPOCH 6 done: loss -4.2812 - lr 0.1000 - bad epochs 0\n",
      "2019-04-17 14:58:24,784 DEV  : loss 0.67968750 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 14:58:34,037 TEST : loss -29.13281250 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 14:58:38,224 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:01:37,757 epoch 7 - iter 0/1 - loss 21.27343750\n",
      "2019-04-17 15:01:37,772 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:01:37,772 EPOCH 7 done: loss 21.2734 - lr 0.1000 - bad epochs 0\n",
      "2019-04-17 15:01:47,711 DEV  : loss 0.67187500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:01:57,094 TEST : loss -24.98437500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:01:59,159 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:04:58,983 epoch 8 - iter 0/1 - loss -14.95312500\n",
      "2019-04-17 15:04:59,002 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:04:59,003 EPOCH 8 done: loss -14.9531 - lr 0.1000 - bad epochs 1\n",
      "2019-04-17 15:05:08,395 DEV  : loss -0.39062500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:05:17,559 TEST : loss 40.95312500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:05:21,811 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:08:20,200 epoch 9 - iter 0/1 - loss -6.23437500\n",
      "2019-04-17 15:08:20,218 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:08:20,219 EPOCH 9 done: loss -6.2344 - lr 0.1000 - bad epochs 0\n",
      "2019-04-17 15:08:29,472 DEV  : loss -0.59375000 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:08:39,038 TEST : loss -1.62500000 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:08:41,411 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:11:39,168 epoch 10 - iter 0/1 - loss -5.31250000\n",
      "2019-04-17 15:11:39,183 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:11:39,183 EPOCH 10 done: loss -5.3125 - lr 0.1000 - bad epochs 1\n",
      "2019-04-17 15:11:48,694 DEV  : loss -0.40625000 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:11:58,222 TEST : loss -11.60937500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:12:00,431 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:14:57,871 epoch 11 - iter 0/1 - loss -0.28125000\n",
      "2019-04-17 15:14:57,890 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:14:57,891 EPOCH 11 done: loss -0.2812 - lr 0.1000 - bad epochs 2\n",
      "2019-04-17 15:15:06,979 DEV  : loss 0.57812500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:15:16,244 TEST : loss -2.25000000 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:15:18,494 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:18:18,237 epoch 12 - iter 0/1 - loss 6.32812500\n",
      "2019-04-17 15:18:18,253 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:18:18,253 EPOCH 12 done: loss 6.3281 - lr 0.1000 - bad epochs 3\n",
      "2019-04-17 15:18:27,689 DEV  : loss 0.79687500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:18:36,747 TEST : loss 27.15625000 - f-score 0.0000 - acc 0.0000\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2019-04-17 15:18:38,829 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:21:40,185 epoch 13 - iter 0/1 - loss -20.21875000\n",
      "2019-04-17 15:21:40,201 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:21:40,201 EPOCH 13 done: loss -20.2188 - lr 0.0500 - bad epochs 0\n",
      "2019-04-17 15:21:49,460 DEV  : loss 0.15625000 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:21:59,538 TEST : loss -21.28125000 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:22:04,110 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:25:00,412 epoch 14 - iter 0/1 - loss -25.17187500\n",
      "2019-04-17 15:25:00,429 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:25:00,429 EPOCH 14 done: loss -25.1719 - lr 0.0500 - bad epochs 0\n",
      "2019-04-17 15:25:09,613 DEV  : loss 0.93750000 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:25:19,380 TEST : loss -34.53125000 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:25:24,056 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:28:28,606 epoch 15 - iter 0/1 - loss -42.65625000\n",
      "2019-04-17 15:28:28,625 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-17 15:28:28,625 EPOCH 15 done: loss -42.6562 - lr 0.0500 - bad epochs 0\n",
      "2019-04-17 15:28:37,709 DEV  : loss -0.42187500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:28:47,102 TEST : loss 8.98437500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:28:51,375 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:31:52,043 epoch 16 - iter 0/1 - loss 36.12500000\n",
      "2019-04-17 15:31:52,059 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:31:52,059 EPOCH 16 done: loss 36.1250 - lr 0.0500 - bad epochs 0\n",
      "2019-04-17 15:32:01,543 DEV  : loss -0.09375000 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:32:10,664 TEST : loss 37.59375000 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:32:12,828 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:35:09,250 epoch 17 - iter 0/1 - loss 25.51562500\n",
      "2019-04-17 15:35:09,268 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:35:09,269 EPOCH 17 done: loss 25.5156 - lr 0.0500 - bad epochs 1\n",
      "2019-04-17 15:35:18,937 DEV  : loss 0.32812500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:35:28,462 TEST : loss 43.79687500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:35:30,831 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:38:31,248 epoch 18 - iter 0/1 - loss -49.65625000\n",
      "2019-04-17 15:38:31,267 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-17 15:38:31,267 EPOCH 18 done: loss -49.6562 - lr 0.0500 - bad epochs 2\n",
      "2019-04-17 15:38:41,540 DEV  : loss 0.39062500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:38:51,744 TEST : loss -34.04687500 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-17 15:38:56,361 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.training_utils import EvaluationMetric\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-ner',\n",
    "              EvaluationMetric.MICRO_F1_SCORE,\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=150,\n",
    "              checkpoint=True)\n",
    "\n",
    "# 8. stop training at any point\n",
    "\n",
    "# 9. continue trainer at later point\n",
    "from pathlib import Path\n",
    "\n",
    "trainer = ModelTrainer.load_from_checkpoint(Path('resources/taggers/example-ner/checkpoint.pt'), 'SequenceTagger', corpus)\n",
    "trainer.train('resources/taggers/example-ner',\n",
    "              EvaluationMetric.MICRO_F1_SCORE,\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=150,\n",
    "              checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. find learning rate\n",
    "learning_rate_tsv = ModelTrainer.find_learning_rate('resources/taggers/example-ner',\n",
    "                                                    'learning_rate.tsv')\n",
    "\n",
    "# 11. plot the learning rate finder curve\n",
    "plotter = Plotter()\n",
    "plotter.plot_learning_rate(learning_rate_tsv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
